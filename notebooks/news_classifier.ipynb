{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Article Classifier\n",
    "This notebook contains all the functionality for classifying news articles, including:\n",
    "- Text preprocessing\n",
    "- Model training\n",
    "- Article classification\n",
    "- Key phrase extraction\n",
    "- Context analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from langdetect import detect\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Feature Extractor\n",
    "This class extracts context-specific features from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.contexts = {\n",
    "            'tech': {\n",
    "                'keywords': ['technology', 'tech', 'ai', 'software', 'hardware', 'digital', 'innovation', \n",
    "                           'smartphone', 'iphone', 'android', 'app', 'computer', 'artificial intelligence',\n",
    "                           'machine learning', 'cloud', 'cybersecurity', '5g', 'blockchain', 'mobile',\n",
    "                           'device', 'platform', 'algorithm', 'interface', 'processor', 'chip'],\n",
    "                'companies': ['apple', 'google', 'microsoft', 'amazon', 'meta', 'tesla', 'nvidia', \n",
    "                            'samsung', 'intel', 'ibm', 'oracle', 'cisco', 'qualcomm', 'adobe']\n",
    "            },\n",
    "            'business': {\n",
    "                'keywords': ['earnings', 'revenue', 'profit', 'market', 'stock', 'shares', 'investors',\n",
    "                           'quarterly', 'financial', 'economy', 'growth', 'sales', 'trading', 'price',\n",
    "                           'investment', 'dividend', 'merger', 'acquisition', 'fiscal', 'shareholder'],\n",
    "                'terms': ['q1', 'q2', 'q3', 'q4', 'year-over-year', 'yoy', 'quarter', 'fiscal']\n",
    "            },\n",
    "            'sports': {\n",
    "                'keywords': ['game', 'match', 'tournament', 'championship', 'league', 'score', 'win',\n",
    "                           'victory', 'team', 'player', 'season', 'coach', 'stadium', 'sports'],\n",
    "                'terms': ['goal', 'points', 'referee', 'injury', 'transfer', 'contract']\n",
    "            },\n",
    "            'entertainment': {\n",
    "                'keywords': ['movie', 'film', 'show', 'music', 'album', 'celebrity', 'actor', 'actress',\n",
    "                           'director', 'performance', 'award', 'entertainment', 'concert', 'premiere'],\n",
    "                'terms': ['box office', 'rating', 'review', 'star', 'episode', 'season']\n",
    "            },\n",
    "            'politics': {\n",
    "                'keywords': ['government', 'policy', 'election', 'political', 'minister', 'president',\n",
    "                           'congress', 'senate', 'law', 'legislation', 'vote', 'campaign', 'party'],\n",
    "                'terms': ['bill', 'reform', 'regulation', 'democratic', 'republican', 'parliament']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_context_features(self, text):\n",
    "        text_lower = text.lower()\n",
    "        features = {}\n",
    "        \n",
    "        for context, indicators in self.contexts.items():\n",
    "            # Keyword score\n",
    "            keyword_matches = sum(1 for keyword in indicators['keywords'] \n",
    "                                if keyword in text_lower)\n",
    "            features[f'{context}_keyword_score'] = keyword_matches\n",
    "            \n",
    "            # Special terms score\n",
    "            if 'terms' in indicators:\n",
    "                term_matches = sum(1 for term in indicators['terms'] \n",
    "                                 if term in text_lower)\n",
    "                features[f'{context}_term_score'] = term_matches\n",
    "            \n",
    "            # Company/Entity score (for tech)\n",
    "            if 'companies' in indicators:\n",
    "                company_matches = sum(1 for company in indicators['companies'] \n",
    "                                    if company in text_lower)\n",
    "                features[f'{context}_company_score'] = company_matches * 2  # Weight companies more\n",
    "                \n",
    "        return features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features_list = []\n",
    "        for text in X:\n",
    "            features = self.get_context_features(text)\n",
    "            features_list.append(features)\n",
    "        return pd.DataFrame(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrase Extraction\n",
    "Function to extract important phrases from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_phrases(text, lang='en', top_n=8):\n",
    "    try:\n",
    "        # Tokenize and clean\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Use appropriate stopwords based on language\n",
    "        try:\n",
    "            if lang == 'fr':\n",
    "                stop_words = set(stopwords.words('french'))\n",
    "            else:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "        except:\n",
    "            stop_words = set()\n",
    "        \n",
    "        # Add custom stopwords\n",
    "        custom_stops = {'said', 'says', 'will', 'would', 'could', 'may', 'might', 'also', 'one', 'two', 'three', 'new'}\n",
    "        stop_words.update(custom_stops)\n",
    "        \n",
    "        # Extract words and bigrams\n",
    "        words = [word for word in tokens if re.match(r'^[a-zA-ZÀ-ÿ]+$', word) and word not in stop_words]\n",
    "        bigrams = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
    "        \n",
    "        # Combine and get frequency\n",
    "        all_phrases = words + bigrams\n",
    "        freq_dist = nltk.FreqDist(all_phrases)\n",
    "        \n",
    "        # Get top phrases\n",
    "        phrases = [(phrase, count) for phrase, count in freq_dist.most_common(top_n * 2)\n",
    "                  if len(phrase) > 1][:top_n]\n",
    "        \n",
    "        return phrases\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Function to train the enhanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enhanced_models():\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv('bbc-text-cleaned.csv')\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['text'], df['category'], \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=df['category']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining enhanced models...\")\n",
    "    \n",
    "    # Create enhanced pipeline\n",
    "    enhanced_pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                ngram_range=(1, 2),\n",
    "                stop_words='english'\n",
    "            )),\n",
    "            ('context', ContextFeatureExtractor())\n",
    "        ])),\n",
    "        ('classifier', GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train models\n",
    "    enhanced_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Create ensemble\n",
    "    enhanced_ensemble = VotingClassifier(estimators=[\n",
    "        ('gb', enhanced_pipeline),\n",
    "        ('lr', Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "                ('context', ContextFeatureExtractor())\n",
    "            ])),\n",
    "            ('classifier', LogisticRegression(max_iter=1000))\n",
    "        ])),\n",
    "        ('svm', Pipeline([\n",
    "            ('features', FeatureUnion([\n",
    "                ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "                ('context', ContextFeatureExtractor())\n",
    "            ])),\n",
    "            ('classifier', LinearSVC(max_iter=1000))\n",
    "        ]))\n",
    "    ], voting='hard')\n",
    "    \n",
    "    enhanced_ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\nSaving models...\")\n",
    "    joblib.dump(enhanced_pipeline, 'enhanced_models/gradient_boosting_model.joblib')\n",
    "    joblib.dump(enhanced_ensemble, 'enhanced_models/ensemble_model.joblib')\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(f\"Gradient Boosting Accuracy: {enhanced_pipeline.score(X_test, y_test):.4f}\")\n",
    "    print(f\"Ensemble Accuracy: {enhanced_ensemble.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Classification\n",
    "Function to classify a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_article(text, model_name='ensemble'):\n",
    "    # Load models\n",
    "    try:\n",
    "        model = joblib.load(f'enhanced_models/{model_name}_model.joblib')\n",
    "    except:\n",
    "        print(\"Error loading model\")\n",
    "        return None\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict([text])[0]\n",
    "    \n",
    "    # Get confidence scores\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probs = model.predict_proba([text])[0]\n",
    "        confidence_scores = dict(zip(model.classes_, probs))\n",
    "    else:\n",
    "        confidence_scores = None\n",
    "    \n",
    "    # Get key phrases\n",
    "    key_phrases = get_key_phrases(text)\n",
    "    \n",
    "    # Get context analysis\n",
    "    context_extractor = ContextFeatureExtractor()\n",
    "    context_scores = context_extractor.get_context_features(text)\n",
    "    \n",
    "    return {\n",
    "        'category': prediction,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'key_phrases': key_phrases,\n",
    "        'context_scores': context_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example article\n",
    "article = \"\"\"\n",
    "Apple reports record quarterly earnings as iPhone sales surge in emerging markets. \n",
    "The tech giant saw a 15% increase in revenue, largely driven by strong performance \n",
    "in India and Southeast Asia. CEO Tim Cook announced plans for expanding their AI initiatives.\n",
    "\"\"\"\n",
    "\n",
    "# Classify article\n",
    "result = classify_article(article)\n",
    "\n",
    "# Print results\n",
    "print(f\"Category: {result['category']}\\n\")\n",
    "\n",
    "print(\"Confidence Scores:\")\n",
    "for category, score in result['confidence_scores'].items():\n",
    "    print(f\"{category}: {score:.2%}\")\n",
    "\n",
    "print(\"\\nKey Phrases:\")\n",
    "for phrase, count in result['key_phrases']:\n",
    "    print(f\"- {phrase} ({count} occurrences)\")\n",
    "\n",
    "print(\"\\nContext Analysis:\")\n",
    "for context, score in result['context_scores'].items():\n",
    "    if score > 0:\n",
    "        print(f\"- {context}: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

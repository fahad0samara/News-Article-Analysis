{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC News Article Classifier\n",
    "This notebook uses the BBC News dataset for training and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from langdetect import detect\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BBC News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BBC dataset\n",
    "df = pd.read_csv('bbc-text-cleaned.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Display sample articles from each category\n",
    "print(\"\\nSample articles from each category:\")\n",
    "for category in df['category'].unique():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(df[df['category'] == category]['text'].iloc[0][:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Feature Extractor\n",
    "Enhanced feature extraction with BBC-specific context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.contexts = {\n",
    "            'tech': {\n",
    "                'keywords': ['technology', 'tech', 'ai', 'software', 'hardware', 'digital', 'innovation', \n",
    "                           'smartphone', 'iphone', 'android', 'app', 'computer', 'artificial intelligence',\n",
    "                           'machine learning', 'cloud', 'cybersecurity', '5g', 'blockchain', 'mobile',\n",
    "                           'device', 'platform', 'algorithm', 'interface', 'processor', 'chip'],\n",
    "                'companies': ['apple', 'google', 'microsoft', 'amazon', 'meta', 'tesla', 'nvidia', \n",
    "                            'samsung', 'intel', 'ibm', 'oracle', 'cisco', 'qualcomm', 'adobe']\n",
    "            },\n",
    "            'business': {\n",
    "                'keywords': ['earnings', 'revenue', 'profit', 'market', 'stock', 'shares', 'investors',\n",
    "                           'quarterly', 'financial', 'economy', 'growth', 'sales', 'trading', 'price',\n",
    "                           'investment', 'dividend', 'merger', 'acquisition', 'fiscal', 'shareholder'],\n",
    "                'terms': ['q1', 'q2', 'q3', 'q4', 'year-over-year', 'yoy', 'quarter', 'fiscal']\n",
    "            },\n",
    "            'sport': {\n",
    "                'keywords': ['game', 'match', 'tournament', 'championship', 'league', 'score', 'win',\n",
    "                           'victory', 'team', 'player', 'season', 'coach', 'stadium', 'sports'],\n",
    "                'terms': ['goal', 'points', 'referee', 'injury', 'transfer', 'contract']\n",
    "            },\n",
    "            'entertainment': {\n",
    "                'keywords': ['movie', 'film', 'show', 'music', 'album', 'celebrity', 'actor', 'actress',\n",
    "                           'director', 'performance', 'award', 'entertainment', 'concert', 'premiere'],\n",
    "                'terms': ['box office', 'rating', 'review', 'star', 'episode', 'season']\n",
    "            },\n",
    "            'politics': {\n",
    "                'keywords': ['government', 'policy', 'election', 'political', 'minister', 'president',\n",
    "                           'congress', 'senate', 'law', 'legislation', 'vote', 'campaign', 'party'],\n",
    "                'terms': ['bill', 'reform', 'regulation', 'democratic', 'republican', 'parliament']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_context_features(self, text):\n",
    "        text_lower = text.lower()\n",
    "        features = {}\n",
    "        \n",
    "        for context, indicators in self.contexts.items():\n",
    "            # Keyword score\n",
    "            keyword_matches = sum(1 for keyword in indicators['keywords'] \n",
    "                                if keyword in text_lower)\n",
    "            features[f'{context}_keyword_score'] = keyword_matches\n",
    "            \n",
    "            # Special terms score\n",
    "            if 'terms' in indicators:\n",
    "                term_matches = sum(1 for term in indicators['terms'] \n",
    "                                 if term in text_lower)\n",
    "                features[f'{context}_term_score'] = term_matches\n",
    "            \n",
    "            # Company/Entity score (for tech)\n",
    "            if 'companies' in indicators:\n",
    "                company_matches = sum(1 for company in indicators['companies'] \n",
    "                                    if company in text_lower)\n",
    "                features[f'{context}_company_score'] = company_matches * 2\n",
    "                \n",
    "        return features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features_list = []\n",
    "        for text in X:\n",
    "            features = self.get_context_features(text)\n",
    "            features_list.append(features)\n",
    "        return pd.DataFrame(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_phrases(text, lang='en', top_n=8):\n",
    "    try:\n",
    "        # Tokenize and clean\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Use appropriate stopwords\n",
    "        try:\n",
    "            if lang == 'fr':\n",
    "                stop_words = set(stopwords.words('french'))\n",
    "            else:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "        except:\n",
    "            stop_words = set()\n",
    "        \n",
    "        # Add custom stopwords\n",
    "        custom_stops = {'said', 'says', 'will', 'would', 'could', 'may', 'might', 'also'}\n",
    "        stop_words.update(custom_stops)\n",
    "        \n",
    "        # Extract words and bigrams\n",
    "        words = [word for word in tokens if re.match(r'^[a-zA-ZÀ-ÿ]+$', word) and word not in stop_words]\n",
    "        bigrams = [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
    "        \n",
    "        # Combine and get frequency\n",
    "        all_phrases = words + bigrams\n",
    "        freq_dist = nltk.FreqDist(all_phrases)\n",
    "        \n",
    "        # Get top phrases\n",
    "        phrases = [(phrase, count) for phrase, count in freq_dist.most_common(top_n * 2)\n",
    "                  if len(phrase) > 1][:top_n]\n",
    "        \n",
    "        return phrases\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Enhanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['category'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['category']\n",
    ")\n",
    "\n",
    "# Create enhanced pipeline\n",
    "enhanced_pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )),\n",
    "        ('context', ContextFeatureExtractor())\n",
    "    ])),\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Create ensemble\n",
    "enhanced_ensemble = VotingClassifier(estimators=[\n",
    "    ('gb', enhanced_pipeline),\n",
    "    ('lr', Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "            ('context', ContextFeatureExtractor())\n",
    "        ])),\n",
    "        ('classifier', LogisticRegression(max_iter=1000))\n",
    "    ])),\n",
    "    ('svm', Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "            ('context', ContextFeatureExtractor())\n",
    "        ])),\n",
    "        ('classifier', LinearSVC(max_iter=1000))\n",
    "    ]))\n",
    "], voting='hard')\n",
    "\n",
    "print(\"Training Gradient Boosting model...\")\n",
    "enhanced_pipeline.fit(X_train, y_train)\n",
    "print(f\"Gradient Boosting Accuracy: {enhanced_pipeline.score(X_test, y_test):.4f}\")\n",
    "\n",
    "print(\"\\nTraining Ensemble model...\")\n",
    "enhanced_ensemble.fit(X_train, y_train)\n",
    "print(f\"Ensemble Accuracy: {enhanced_ensemble.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Save models\n",
    "print(\"\\nSaving models...\")\n",
    "joblib.dump(enhanced_pipeline, 'enhanced_models/gradient_boosting_model.joblib')\n",
    "joblib.dump(enhanced_ensemble, 'enhanced_models/ensemble_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_article(text, model):\n",
    "    # Make prediction\n",
    "    prediction = model.predict([text])[0]\n",
    "    probs = model.predict_proba([text])[0]\n",
    "    \n",
    "    # Get confidence scores\n",
    "    confidence_scores = dict(zip(model.classes_, probs))\n",
    "    \n",
    "    # Get key phrases\n",
    "    key_phrases = get_key_phrases(text)\n",
    "    \n",
    "    # Get context analysis\n",
    "    context_extractor = ContextFeatureExtractor()\n",
    "    context_scores = context_extractor.get_context_features(text)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Predicted Category: {prediction}\\n\")\n",
    "    \n",
    "    print(\"Confidence Scores:\")\n",
    "    for category, score in sorted(confidence_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{category}: {score:.2%}\")\n",
    "    \n",
    "    print(\"\\nKey Phrases:\")\n",
    "    for phrase, count in key_phrases:\n",
    "        print(f\"- {phrase} ({count} occurrences)\")\n",
    "    \n",
    "    print(\"\\nContext Analysis:\")\n",
    "    for context, score in context_scores.items():\n",
    "        if score > 0:\n",
    "            print(f\"- {context}: {score}\")\n",
    "\n",
    "# Test articles\n",
    "test_articles = [\n",
    "    # Mixed Tech/Business\n",
    "    \"\"\"\n",
    "    Apple reports record quarterly earnings as iPhone sales surge in emerging markets. \n",
    "    The tech giant saw a 15% increase in revenue, largely driven by strong performance \n",
    "    in India and Southeast Asia. CEO Tim Cook announced plans for expanding their AI initiatives.\n",
    "    \"\"\",\n",
    "    \n",
    "    # Pure Tech\n",
    "    \"\"\"\n",
    "    Microsoft unveils groundbreaking AI features for Windows 12, integrating advanced \n",
    "    machine learning capabilities across the operating system. The new update includes \n",
    "    real-time language translation and predictive task automation.\n",
    "    \"\"\",\n",
    "    \n",
    "    # Pure Business\n",
    "    \"\"\"\n",
    "    Goldman Sachs reports Q4 earnings beating market expectations, with revenue up 25% \n",
    "    year-over-year. The investment bank saw strong growth in trading and investment \n",
    "    banking divisions, leading to increased shareholder dividends.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"Testing with Gradient Boosting model:\")\n",
    "print(\"=\" * 50)\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    print(f\"\\nTest Article {i}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Text: {article.strip()}\\n\")\n",
    "    analyze_article(article, enhanced_pipeline)\n",
    "\n",
    "print(\"\\n\\nTesting with Ensemble model:\")\n",
    "print(\"=\" * 50)\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    print(f\"\\nTest Article {i}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Text: {article.strip()}\\n\")\n",
    "    analyze_article(article, enhanced_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_your_article():\n",
    "    article = input(\"Enter your news article: \")\n",
    "    print(\"\\nGradient Boosting Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    analyze_article(article, enhanced_pipeline)\n",
    "    \n",
    "    print(\"\\nEnsemble Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    analyze_article(article, enhanced_ensemble)\n",
    "\n",
    "# Uncomment and run this cell to test your own article\n",
    "# test_your_article()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
